{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We Rate Dogs - Data Wrangling Report\n",
    "##### Udacity Data Analysis NanoDegree Program\n",
    "\n",
    "> Author: Chris Bartsch<br>\n",
    "> Date: August 26, 2018\n",
    "\n",
    "\n",
    "## Data Wrangling Approach to the We Rate Dogs Data\n",
    "\n",
    "The dog rating data proved to be a challenging set of data to analyze and clean due to the \"openness\" of the data.  There are limited rules which guide the dog rating process and this lead to several issues which needed to be resolved.  Overall, I was able to generate a relatively clean data set.  However, it should be noted that it would take much longer to truly clean all the data.  \n",
    "\n",
    "### Data Collection\n",
    "\n",
    "Data was loaded from all three data sources, using three different methods: a) read from local csv; b) download the predictions file using the response library and c) setting up a twitter developer account to access twitter data to supplement the other data -- primarily favorites and retweet information.\n",
    "\n",
    "### Tidiness\n",
    "\n",
    "I used the approach of merging data from all three data sets into a single data frame.  Since all data from the three sources was data about each tweet, I felt it provided a more concise view of the data.  The twitter_archive_enhanced csv file was used as the main data frame and other sources were merged into it.  \n",
    "\n",
    "Besides merging data, there were a few other areas which were tidied up to make quality clean up easier.  \n",
    "\n",
    "-  Rather than storing both a numerator and denominator, a new rating column was created based on a standard denominator of 10.  While the open rating system is certainly part of the allure of the WeRateDogs twitter account, the variance makes it difficult to produce meaningful results in the insights and visualization stage.\n",
    "-  Dog stage data was melted into a single column.  There were a few records which had two different stages listed and those were cleaned based on some assumptions and some manual research.\n",
    "-  A general cleanup was done to remove tweets missing images, and those which were not original tweets (retweets and replies)\n",
    "\n",
    "### Quality\n",
    "\n",
    "I found and cleaned 10 different quality issues through the wrangling process.  Many of those issues did not impact the final visualizations I created but they help in creating a final data set which was much more concise and easier to understand.  The quality issues can be rolled up into the following xxx categories.\n",
    "\n",
    "-  Data Types:  Data type updates were necessary to merge the data retrieved through the twitter API.  Also when image prediction data was merged, the boolean data types were converted to objects -- Those were set back to boolean.\n",
    "-  Missing Data:  This was really the most significant portion of the cleaning process.  Dog names and dog stages both required quite a bit of work to determine the extent of the issues and how to fix them.  For example, the method used to extract dog names from the text field used key phrases such as \"This is [dog name]\" but did not account for someone tweeting \"This is a...\" or \"This is the...\".\n",
    "-  Data inconsistencies: There were inconsistencies found in dog breed data from image predictions and I also discovered that the expanded_url column contained duplicated URLs and in many cases, URLs which were not related to dog rating.\n",
    "\n",
    "### Cleaning\n",
    "\n",
    "Data cleaning required both programmatic and manual means to ensure data quality.  There were times which a manual view of the actual tweet on twitter was required to get information on the rating or do stage information. This was especially true when multiple dogs were included in a single tweet.  No data was cleaned using Excel or functions outside of a Jupyter Notebook.\n",
    "\n",
    "### Final Output\n",
    "\n",
    "The resulting final dataframe was stored locally as twitter_archive_master.csv and was used for the visualizations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
